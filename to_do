#------------TO DO------------#

PRIO HIGH

correct to sort descending for gbc score?
correct to sort ascending for gbr score?

try again with normed t test : getting .85 .90 accuracy?

data augmentation sklearn

recursive feature selection

train not scaled data

understand who GBR and GBC work for presentation...

PRIO MID
try more cv_folds more accuracy? -> cv10 did worse than cv5?

check if GridSearchCV fully taken advantage of?

try some other scoring metrics

Free Surfer Group Analysis

PRIO LOW
use p_values instead? probably doesn't bring much
write Alessandro for his preproc steps, cut down on FS_feats?

#------------DONE------------#
- (train not scaled data) remove FS_feats that are sparse or non-informative, or close to zero
bug: why not writing FS_features to excel? -> .save()
why train_test_split on std always same 8 t features? -> is indeed the right calculation
check if GridSearchCV used properly -> indeed
try imbalanced functionality of gridsearch scoring with more training and test samples
t and p values or and? -> and
try just with minMax
use only severe YBOCS pats for t feat selection

#------------ IF TIME LATER------------#
graph model scores against num of features?
run meta analysis on them? which features seen the most?
deep learning on original images

#------------LEARNING------------#
ml experience : learning : can build up insights for further more focused analysis i.e. t feats inform further learning
or simple clustering informs what features to focus on ... and reduces much searchin in hypothesis space

implies doing experiments in steps for faster feedback and informing next ecps instead of doing many experiments at a time...

using only severe YBOCS for t test seems to make accuracy worse

using hand picked balanced