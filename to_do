#------------TO DO------------#

PRIO HIGH-------------

to do :
adding permutation importance ranking of features at end
switch to index storage of features from pat_frame sys.getsizeof(object) reveals typical feature strings more
than 2x larger...

to understand :
XGboost parameters,
mutual info

PAPER:
contribution is perhaps new pipeline (also for paper) over clf reg supervised learning tasks over various objectives...
(2, 3, 4 class)
# non-ocd (0-9) mild (10-20), moderate (21-30), severe (31-40) Okasha et. al. (2000)
feature set loop
hyperparameter loop
cross validation loop
for each feature set report best scoring model (with hyperparameters)...

double up on atlas to
1.include more features
2.perhaps account for error in freesurfer for area

analysis of complexity savings

lsvm one v rest and not one vs one
roc_auc_score binary classification lsvc is weighted average for imbalanced classes

always strategy from infinite space with statistical strategies to narrow down space
-menze with t test
-rankings..

Boedhoe no cortical thickness or area correlations in severity?  maybe i find something different

section on (main purpose is to have measure of power for feature subset
proposal for new pipeline RFECV over grid remove extraneous feats and differences between two atlases
with freq_item_set mining with relatively high support
retrain on grid, find best5 est and predictions and assign these prediction values as importance/power to
freq item sets with perm importances for ests with predictions over 0.5 sorted
but would have for each class target many subsets (+2 boedhoe, hoexter) with same under data structures,
! computational complexity increases by how much - calculate...
how to summarize over targets and sets?  to create final score for comparison?

factors of set's power
number of times set occurs over tgts
prediction performance
-> simple multiplication of the two
can compute also permutation importance over feats from all remaining feat sets


new pipeline:
1.feature pool :per task: pool union of t test between mild and severe, p_value based f_scores, and mutual information
2.feature subset collection from pool, forward permutation importance backward rfecv reporting on held out test set and
compared with hoexter and boedhoe sets
-(further work) include more subsets by taking not just best but perhaps best 5
3.frequent item set analysis
4. perhaps NB explanation this week

PRIO MID-------------

make dataset Subs class for just one type of group: so pat = Subs(...group='pat') con = Subs(...group='con')
rename to train_frame, train_norm_frame pat.train_norm_frame...(Naming Guideline: Important Things First, Type Hint last)
check for feat sort and name compatibility with function in class pat.check(con)...
make Subs invariant to resample() w.r.t random features already in there, but don't need for pipeline

create cross validation loop with correction of covariates, resampling, and other desired transformatinos like
permutation importance, rfe strategies

add in smart hyperparameter search using bayesion methods

implement scrum board
-keep track what to do next
-what done stack

PRIO LOW-------------
run bash fs_gen_tables in python with parameters desikan destrieux all volume area thickness

Free Surfer Group Analysis


#------------DONE STACK------------#
good idea to do univariate feature analysis on augmented OR vanilla training set? augmented, because at least
the f and mi correlate with targets, and better when balanced targets (and more) also statistical view aligns with
data trained on, so...

ask bjoern if need to base on prediction values, can just use best val_scores : bjoern looked at pipeline and didnt say
anything knowing ocd data set not so good with prediction...

understand how rfe in the cv, thanks to bjoern : feature set candle sticks (created from cv loops)
on curve of features to performance, choose 1 std dev above mean at given feat set point
and draw horizontal line backwards and reduce to intersect point feature set with same performance

add prediction confidence scores
look at best loss functions for each estimator, roc_auc metric for binary classification (need class probabilities)
dont understand why cv_folds isn't calculated s.t. in each training fold, est sees same number of each class -> greater...
cv_folds smaller val set with few of each class and greater train set with more of all classes
self programmed permutation importance holding fixed control variables
is it the same to take the best rfecv and perm tested models and the forward prediction training models -> yes (what?)
make dataset class like sklearn to call it up for different analyses
check if GridSearchCV fully taken advantage of?
randomized search with more than 10 n_iter samplings
smote is more standard
try more cv_folds more accuracy? -> cv10 did worse than cv5? always trying new ones
try data augmentation with repeating samples -> seems to work best out of SMOTE and ADASYN
median class split wo univariate classification -> broke into 2 classes and changed to 0, 1, ... instead of 1,2,3 ...more sklearn style
just use standard atlas freesurfer, just volume, area, and/or thickness -> added functionality for this with ttest
only thickness to see if enhance boedhoe and hoexter (informed by orbital thickness what found in previous
experiments) - not seeming to improve
try on hand picked test set and then rest with imbalanced and resamp
added easy parameter setting for resampled sets in running code
recursive feature selection

tried ADASYN_clf with t_clf -> results much worse
try again with normed t test : getting .85 .90 accuracy? doesnt make sense with norm because dist all same 0 mean 1 std
correct to sort descending for gbc score? yes
correct to sort ascending for gbr score? yes
- (train not scaled data) remove FS_feats that are sparse or non-informative, or close to zero
bug: why not writing FS_features to excel? -> .save()
why train_test_split on std always same 8 t features? -> is indeed the right calculation
check if GridSearchCV used properly -> indeed
try imbalanced functionality of gridsearch scoring with more training and test samples
t and p values or and? -> and
try just with minMax
use only severe YBOCS pats for t feat selection

#------------ IF TIME LATER------------#
graph model scores against num of features?
run meta analysis on them? which features seen the most?
deep learning on original images
