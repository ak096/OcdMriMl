#------------TO DO------------#

PRIO HIGH-------------

new pipeline:
1.feature pool :per task: pool union of t test between mild and severe, p_value based f_scores, and mutual information
2.feature subset collection from pool, forward permutation importance backward rfecv reporting on held out test set and
compared with hoexter and boedhoe sets
-(further work) include more subsets by taking not just best but perhaps best 5
3.frequent item set analysis
4. perhaps NB explanation this week

to understand :
good idea to do univariate feature analysis on augmented OR vanilla training set?

to do :
add prediction confidence scores

understand
XGboost parameters,
mutual info
how perm importance during cv





PAPER:
contribution is perhaps new pipeline (also for paper) over clf reg supervised learning tasks over various objectives...
(2, 3, 4 class)
# non-ocd (0-9) mild (10-20), moderate (21-30), severe (31-40) Okasha et. al. (2000)
feature set loop
hyperparameter loop
cross validation loop
for each feature set report best scoring model (with hyperparameters)...

lsvm one v rest and not one vs one
roc_auc_score binary classification lsvc is weighted average for imbalanced classes

always strategy from infinite space with statistical strategies to narrow down space
-menze with t test
-rankings..

Boedhoe no cortical thickness or area correlations in severity?  maybe i find something different



PRIO MID-------------


create cross validation loop with correction of covariates, resampling, and other desired transformatinos like
permutation importance, rfe strategies

add in smart hyperparameter search using bayesion methods

PRIO LOW-------------
run bash fs_gen_tables in python with parameters desikan destrieux all volume area thickness

Free Surfer Group Analysis


#------------DONE------------#
look at best loss functions for each estimator, roc_auc metric for binary classification (need class probabilities)
dont understand why cv_folds isn't calculated s.t. in each training fold, est sees same number of each class -> greater...
cv_folds smaller val set with few of each class and greater train set with more of all classes
self programmed permutation importance holding fixed control variables
is it the same to take the best rfecv and perm tested models and the forward prediction training models -> yes (what?)
make dataset class like sklearn to call it up for different analyses
check if GridSearchCV fully taken advantage of?
randomized search with more than 10 n_iter samplings
smote is more standard
try more cv_folds more accuracy? -> cv10 did worse than cv5? always trying new ones
try data augmentation with repeating samples -> seems to work best out of SMOTE and ADASYN
median class split wo univariate classification -> broke into 2 classes and changed to 0, 1, ... instead of 1,2,3 ...more sklearn style
just use standard atlas freesurfer, just volume, area, and/or thickness -> added functionality for this with ttest
only thickness to see if enhance boedhoe and hoexter (informed by orbital thickness what found in previous
experiments) - not seeming to improve
try on hand picked test set and then rest with imbalanced and resamp
added easy parameter setting for resampled sets in running code
recursive feature selection

tried ADASYN_clf with t_clf -> results much worse
try again with normed t test : getting .85 .90 accuracy? doesnt make sense with norm because dist all same 0 mean 1 std
correct to sort descending for gbc score? yes
correct to sort ascending for gbr score? yes
- (train not scaled data) remove FS_feats that are sparse or non-informative, or close to zero
bug: why not writing FS_features to excel? -> .save()
why train_test_split on std always same 8 t features? -> is indeed the right calculation
check if GridSearchCV used properly -> indeed
try imbalanced functionality of gridsearch scoring with more training and test samples
t and p values or and? -> and
try just with minMax
use only severe YBOCS pats for t feat selection

#------------ IF TIME LATER------------#
graph model scores against num of features?
run meta analysis on them? which features seen the most?
deep learning on original images

#------------LEARNING------------#
ml experience : learning : can build up insights for further more focused analysis i.e. t feats inform further learning
or simple clustering informs what features to focus on ... and reduces much searchin in hypothesis space

implies doing experiments in steps for faster feedback and informing next ecps instead of doing many experiments at a time...

using only severe YBOCS for t test seems to make accuracy worse

using hand picked balanced

mathe ist sehen dass
t_feats_num_total = t_frame.shape[1]

t_feats_list = t_frame.columns.tolist()
gleich werte sind, also sich gleichen. aber andere konzepten sind mit unterschiedleich folgen und werten unter
anderen Bedingungen

Sonntag 05.05.2019
scalability concept in concrete design example
strong dependency: variablization : i.e. making size of list in for loop dependent on input size
instead of hard coding its size like l = [] for i in l2: do l.append[something i] instead of
l = [, ,] for idx, i in l2 : do l[idx], automatically scaled with growth or shrinking of l2 ..

change naming convention from related objects, distinguishing name in front (DE principle), e.g. l_feat_sel, instead of
feat_sel_l, distinguishing thing easier to get first serves function
